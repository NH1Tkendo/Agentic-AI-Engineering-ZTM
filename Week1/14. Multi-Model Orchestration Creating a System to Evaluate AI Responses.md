## Đánh Giá Các Mô Hình AI Bằng LM

### Tổng Quan Về Quy Trình
- Sử dụng một mô hình ngôn ngữ (Language Model - LM) để đánh giá các câu trả lời từ các mô hình khác nhau.
- Có hai danh sách: competitors (các đối thủ) chứa tên các mô hình, và answers (các câu trả lời) chứa phản hồi của chúng.
- Mục tiêu: Ghép đôi và đánh giá để xác định mô hình nào tốt nhất, tránh việc đọc thủ công.

### Sử Dụng Hàm Zip Trong Python
- Hàm zip cho phép lặp qua hai danh sách cùng lúc, ghép đôi các phần tử tương ứng.
- Ví dụ mã nguồn:
  ```
  for competitor, answer in zip(competitors, answers):
      print(competitor, answer)
  ```
- Kết quả: In ra từng đối thủ và câu trả lời tương ứng, giúp dễ dàng xem xét.

### Sử Dụng Hàm Enumerate Trong Python
- Hàm enumerate dùng để lặp qua danh sách và theo dõi chỉ số (index) mà không cần biến đếm thủ công.
- Ví dụ mã nguồn:
  ```
  together = ""
  for index, answer in enumerate(answers):
      together += f"Response from competitor {index + 1}:\n{answer}\n\n"
  ```
- Lưu ý: Thêm 1 vào index để bắt đầu từ 1 thay vì 0, làm cho hiển thị thân thiện hơn.
- In ra để kiểm tra lỗi, ví dụ phát hiện sai sót trong quá trình ghép nội dung.

### Xây Dựng Prompt Cho Mô Hình Đánh Giá (Judge)
- Sử dụng chuỗi đa dòng với dấu ngoặc kép ba (triple quotes) để tạo khối văn bản lớn mà không cần nối chuỗi.
- Trong f-string, dùng hai dấu ngoặc nhọn {{ }} để hiển thị ngoặc nhọn thực tế mà không bị diễn giải.
- Nội dung prompt:
  - Giới thiệu: "Bạn đang đánh giá cuộc thi giữa {số lượng} đối thủ."
  - Nhiệm vụ: Đánh giá từng phản hồi dựa trên câu hỏi đã cho.
  - Định dạng phản hồi: Chỉ JSON thuần, với khóa "results" chứa thứ hạng (first_best, second_best, v.v., giá trị là số đối thủ).
  - Cấm thêm định dạng markdown hoặc code block để đảm bảo JSON sạch.
- Ghép các phản hồi từ competitors vào prompt.

### Thực Hiện Đánh Giá
- Đưa prompt vào danh sách messages với cấu trúc chuẩn (system/user).
- Chọn mô hình judge: Sử dụng o3-mini (của OpenAI), đắt hơn nhưng chú ý chi tiết tốt.
- Lưu ý: Không tiết lộ tên mô hình cho judge, chỉ dùng số thứ tự để tránh thiên vị.
- Kết quả JSON: Phân tích và chuyển thành thứ hạng (ví dụ: first_best: 3).
- Xử lý kết quả: Sử dụng enumerate để lặp qua thứ hạng, trừ 1 để lấy chỉ số danh sách, in tên mô hình từ tốt nhất đến kém nhất.

### Kết Quả Đánh Giá
- Thứ hạng mẫu:
  - 1: Gemini 2.0 Flash
  - 2: GPT-4 Mini
  - 3: Llama 3.3
  - 4: DeepSeek
  - 5: Claude 3.7
  - 6: Llama 3.2 (kém nhất, bỏ dở câu trả lời)
- Đây là kết quả không khoa học; có thể cải thiện bằng cách dùng nhiều judge và tính trung bình.

### Phân Tích Mô Hình Agentic Workflow
- Ví dụ này minh họa sự hợp tác giữa các mô hình: Một mô hình tạo câu hỏi, nhiều mô hình trả lời, một mô hình đánh giá.
- Nhận diện pattern: Có thể là hybrid giữa các pattern như reflection (phản ánh), tool use (sử dụng công cụ), hoặc multi-agent collaboration (hợp tác đa agent).
- Bài tập: Xác định pattern đã dùng; chọn một pattern khác (ví dụ: planning, routing) để thêm vào quy trình.

### Mục Tiêu Của Lab
- Thử nghiệm API khác nhau: OpenAI API phổ biến cho nhiều mô hình (trừ Anthropic), cấu trúc prompt cơ bản.
- Thực hành orchestration giữa mô hình: Phân chia vấn đề lớn thành nhỏ, thêm tính tự chủ (autonomy).
- Khuyến khích: Thêm pattern mới, tạo PR vào thư mục community contributions (xóa output notebook trước khi push).

### Ý Nghĩa Thương Mại
- Áp dụng rộng rãi: Gửi yêu cầu đến nhiều mô hình, đánh giá phản hồi để chọn tốt nhất, tăng độ bền vững và chính xác.
- Ví dụ: Tóm tắt văn bản, soạn email, tạo tài liệu yêu cầu kinh doanh.
- Kỹ thuật: Voting (bầu chọn) giữa các phản hồi, feedback loop để cải thiện, giải quyết vấn đề phức tạp hơn.
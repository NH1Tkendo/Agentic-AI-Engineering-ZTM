## Gọi và So sánh Các Mô hình AI: Anthropic, Gemini, DeepSeek, Grok và Llama

### Mục tiêu
- Hiểu cách gọi các mô hình AI khác nhau thông qua API.
- So sánh cách thiết lập và sử dụng các thư viện Python để tương tác với các mô hình như Anthropic (Claude), Gemini, DeepSeek, Grok và Llama.
- Nhấn mạnh sự tương thích của các API với thư viện của OpenAI và cách triển khai chúng.

---

### 1. Anthropic (Claude 3.7 Sonnet)
#### Khái niệm
- **Anthropic API** được sử dụng để gọi mô hình Claude 3.7 Sonnet, một trong những mô hình mạnh mẽ nhất hiện nay.
- API của Anthropic có một số điểm khác biệt so với chuẩn OpenAI, đặc biệt là yêu cầu chỉ định số lượng token tối đa (`maximum number of tokens`) mà mô hình được phép sinh ra.

#### Cách thực hiện
- Tạo một instance của thư viện Python `anthropic` và đặt tên là `Claude`.
- Gọi hàm `Claude.create` với các tham số:
  - Tên mô hình (`model name`).
  - Danh sách tin nhắn (`messages`).
  - Số lượng token tối đa (`max_tokens`).
- Kết quả trả về được hiển thị trực tiếp.

#### Đặc điểm
- Câu trả lời từ Claude ngắn gọn, rõ ràng và dễ hiểu.
- Tốc độ xử lý nhanh hơn một số mô hình khác do tính súc tích.

---

### 2. Google Gemini
#### Khái niệm
- **Gemini** là mô hình AI của Google, được gọi thông qua API tương thích với chuẩn của OpenAI.
- Thư viện Python của OpenAI được sử dụng để gọi Gemini bằng cách chỉ định một URL cơ sở (`base URL`) của Google.

#### Cách thực hiện
- Tạo instance của thư viện OpenAI, nhưng thay vì sử dụng endpoint của OpenAI, chỉ định endpoint của Google (kết thúc bằng `/openai` để tương thích).
- Sử dụng khóa API của Google (`Google API key`).
- Gọi hàm `Gemini.create` với:
  - Tên mô hình: `Gemini 2.0 Flash`.
  - Danh sách tin nhắn (`messages`).
- Kết quả trả về được hiển thị.

#### Đặc điểm
- Câu trả lời dài, chi tiết, bao gồm các chiến lược giảm thiểu (`mitigation strategies`) và khung đánh giá đạo đức (`framework for ethical assessment`).
- Tương thích cao với thư viện OpenAI, giúp dễ dàng tích hợp.

#### Ghi chú
- Google cung cấp endpoint tương thích với chuẩn OpenAI, cho phép sử dụng thư viện OpenAI mà không cần thay đổi nhiều mã.

---

### 3. DeepSeek
#### Khái niệm
- **DeepSeek** cung cấp mô hình lớn với 671 tỷ tham số (`671 billion parameters`), được gọi qua API tương thích với OpenAI.
- DeepSeek chỉ hỗ trợ sử dụng thư viện OpenAI, không có thư viện riêng.

#### Cách thực hiện
- Tạo instance của thư viện OpenAI, chỉ định URL cơ sở của DeepSeek.
- Sử dụng khóa API của DeepSeek.
- Gọi hàm `DeepSeek.create` với:
  - Tên mô hình: `DeepSeek Chat`.
  - Danh sách tin nhắn (`messages`).
- Kết quả trả về được hiển thị và lưu vào danh sách để so sánh.

#### Đặc điểm
- Câu trả lời dài, chi tiết, bao gồm các khung đánh giá (`frameworks`) tương tự Gemini.
- Thời gian xử lý lâu hơn do kích thước mô hình lớn.

#### Ghi chú
- DeepSeek có hai mô hình chính: `DeepSeek Chat` và `DeepSeek Reasoning (R1)`. Trong bài học này, chỉ sử dụng `DeepSeek Chat` để đảm bảo công bằng khi so sánh.

---

### 4. Grok
#### Khái niệm
- **Grok** (viết với chữ “Q”) là mô hình của nhà cung cấp sử dụng phần cứng chuyên dụng để tăng tốc độ suy luận (`fast inference`).
- Hỗ trợ API tương thích với OpenAI, sử dụng mô hình Llama 3.3 với 70 tỷ tham số.

#### Cách thực hiện
- Tạo instance của thư viện OpenAI, chỉ định URL cơ sở của Grok (cũng chứa `/openai` để tương thích).
- Sử dụng khóa API của Grok.
- Gọi hàm `Grok.create` với:
  - Tên mô hình: `Llama 3.3`.
  - Danh sách tin nhắn (`messages`).
- Kết quả trả về được hiển thị.

#### Đặc điểm
- Tốc độ xử lý cực nhanh nhờ phần cứng chuyên dụng.
- Câu trả lời chi tiết, bao gồm các chiến lược giảm thiểu (`mitigation strategies`).
- Mô hình Llama 3.3 có hiệu suất tương đương hoặc vượt trội so với Llama 3.1 (405 tỷ tham số).

---

### 5. Llama (Chạy cục bộ với Ollama)
#### Khái niệm
- **Llama** là mô hình chạy cục bộ trên máy tính cá nhân thông qua phần mềm **Ollama**, cung cấp endpoint tương thích với OpenAI.
- Chỉ phù hợp với các mô hình nhỏ (1.5–8 tỷ tham số) do hạn chế tài nguyên phần cứng.

#### Cách thực hiện
1. **Cài đặt Ollama**:
   - Truy cập `Ollama.com`, tải và cài đặt phần mềm.
   - Chạy lệnh `ollama serve` trong terminal để khởi động dịch vụ.
   - Kiểm tra tại `localhost:11434` để xác nhận Ollama đang chạy.
2. **Tải mô hình**:
   - Sử dụng lệnh `ollama pull llama3.2` để tải mô hình Llama 3.2 (3 tỷ tham số).
   - Tránh sử dụng Llama 3.3 (70 tỷ tham số) vì yêu cầu tài nguyên lớn.
3. **Gọi mô hình**:
   - Tạo instance của thư viện OpenAI, chỉ định URL cơ sở là `localhost:11434`.
   - Khóa API không quan trọng (có thể dùng “ollama”).
   - Gọi hàm `chat.completions.create` với:
     - Tên mô hình: `llama3.2`.
     - Danh sách tin nhắn (`messages`).
   - Kết quả trả về được hiển thị.

#### Đặc điểm
- Câu trả lời từ Llama 3.2 ngắn gọn, chất lượng trung bình, do hạn chế của mô hình nhỏ.
- Phù hợp cho máy tính cá nhân với tài nguyên hạn chế.
- Sử dụng mã C++ tối ưu hóa để xử lý nhanh trên phần cứng cục bộ.

#### Ghi chú
- Các mô hình nhỏ như Llama 3.2 (3 tỷ tham số) hoặc Llama 3.2:1b (1.5 tỷ tham số) là lựa chọn tốt cho chạy cục bộ.
- Các mô hình khác như Qwen, Gemma, Phi hoặc các phiên bản thu gọn của DeepSeek cũng có thể được sử dụng.
- Để xem danh sách mô hình, truy cập tab “Models” trên trang Ollama.

---

### Ghi chú thêm
- **Tương thích API**: Hầu hết các mô hình (trừ Anthropic) sử dụng định dạng API của OpenAI, giúp đơn giản hóa việc tích hợp bằng thư viện OpenAI.
- **So sánh hiệu suất**:
  - Claude: Ngắn gọn, nhanh, rõ ràng.
  - Gemini: Chi tiết, có chiến lược giảm thiểu và khung đạo đức.
  - DeepSeek: Chi tiết, thời gian xử lý lâu hơn.
  - Grok: Nhanh, mạnh mẽ, sử dụng phần cứng chuyên dụng.
  - Llama (Ollama): Phù hợp cho máy cục bộ, nhưng hạn chế về chất lượng do mô hình nhỏ.
- **Lưu ý khi chạy Llama cục bộ**:
  - Tránh mô hình lớn như Llama 3.3 do yêu cầu tài nguyên cao.
  - Sử dụng lệnh `!ollama pull <model_name>` trong notebook để tải mô hình.
  - Đảm bảo Ollama đang chạy trước khi gọi API.

---

### Mã nguồn ví dụ
#### Gọi Claude (Anthropic)
```python
import anthropic

claude = anthropic.Anthropic()
response = claude.create(
    model="claude-3-7-sonnet",
    messages=[{"role": "user", "content": "Câu hỏi của bạn"}],
    max_tokens=1000
)
print(response.content)
```

#### Gọi Gemini (Google)
```python
from openai import OpenAI

gemini = OpenAI(base_url="https://api.google.com/openai", api_key="YOUR_GOOGLE_API_KEY")
response = gemini.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[{"role": "user", "content": "Câu hỏi của bạn"}]
)
print(response.choices[0].message.content)
```

#### Gọi DeepSeek
```python
from openai import OpenAI

deepseek = OpenAI(base_url="https://api.deepseek.com/openai", api_key="YOUR_DEEPSEEK_API_KEY")
response = deepseek.chat.completions.create(
    model="deepseek-chat",
    messages=[{"role": "user", "content": "Câu hỏi của bạn"}]
)
print(response.choices[0].message.content)
```

#### Gọi Grok
```python
from openai import OpenAI

grok = OpenAI(base_url="https://api.grok.ai/openai", api_key="YOUR_GROK_API_KEY")
response = grok.chat.completions.create(
    model="llama-3.3",
    messages=[{"role": "user", "content": "Câu hỏi của bạn"}]
)
print(response.choices[0].message.content)
```

#### Gọi Llama (Ollama)
```python
from openai import OpenAI

llama = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")
response = llama.chat.completions.create(
    model="llama3.2",
    messages=[{"role": "user", "content": "Câu hỏi của bạn"}]
)
print(response.choices[0].message.content)
```

---

### Liên kết chéo
- **Khóa học liên quan**: API AI, Xử lý ngôn ngữ tự nhiên (NLP), Tích hợp mô hình AI.
- **Chủ đề tiếp theo**: So sánh và đánh giá chất lượng câu trả lời từ các mô hình AI.
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AI\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond defining them, elaborate on the fundamental relationship between thermodynamic entropy and information entropy. Then, discuss how this conceptual link either illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age.\n"
     ]
    }
   ],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m openai = OpenAI();\n\u001b[32m      2\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m answer = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m      7\u001b[39m display(Markdown(answer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic-AI-Engineering-ZTM\\agents\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic-AI-Engineering-ZTM\\agents\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic-AI-Engineering-ZTM\\agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic-AI-Engineering-ZTM\\agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "openai = OpenAI();\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down the relationship between thermodynamic entropy and information entropy, and then delve into how this connection might help us understand the fracturing of shared public realities in the digital age.\n",
       "\n",
       "**Thermodynamic Entropy vs. Information Entropy: A Deep Dive**\n",
       "\n",
       "*   **Thermodynamic Entropy (S):**\n",
       "\n",
       "    *   **Definition:**  At its core, thermodynamic entropy, as defined in classical thermodynamics (particularly the second law), is a measure of the *disorder* or *randomness* within a physical system. It quantifies the number of possible microscopic arrangements (microstates) that correspond to a given macroscopic state (macrostate) of the system. A higher entropy means there are more possible arrangements, and the system is more disordered.\n",
       "    *   **Evolution:** The Second Law of Thermodynamics states that the total entropy of a closed (isolated) system *always* increases or remains constant in an idealized reversible process.  In real-world, irreversible processes, entropy always increases.  This reflects a natural tendency toward equilibrium and the dissipation of energy.  Think of ice melting in a warm room.  The highly ordered ice structure transitions to a more disordered state (liquid water), increasing entropy.  The total energy remains the same, but its distribution is more spread out.\n",
       "    *   **Formalism:**  Mathematically, the change in entropy (ΔS) is often defined as  ΔS = ∫dQ/T, where dQ is the heat transferred and T is the absolute temperature. In statistical mechanics (a more microscopic view), entropy is defined by Boltzmann's equation: S = k * ln(W), where k is Boltzmann's constant and W is the number of microstates corresponding to the given macrostate.\n",
       "\n",
       "*   **Information Entropy (H):**\n",
       "\n",
       "    *   **Definition:** Information entropy, pioneered by Claude Shannon in information theory, quantifies the *uncertainty* or *surprise* associated with a random variable or a probability distribution. It measures the average amount of information needed to describe the outcome of a random event. A higher entropy means more uncertainty (less predictability).\n",
       "    *   **Evolution:** Unlike thermodynamic entropy, information entropy doesn't necessarily always increase.  It can increase or decrease, depending on how information is processed or acquired.  For example, observing a coin flip outcome reduces the uncertainty of the outcome (decreases information entropy). Conversely, introducing noise to a signal increases the uncertainty (increases information entropy).\n",
       "    *   **Formalism:**  Shannon's formula for information entropy is H(X) = - Σ p(x) * log2(p(x)), where X is a random variable, p(x) is the probability of outcome x, and the sum is taken over all possible outcomes. The base of the logarithm (usually base 2) determines the units of entropy (bits).\n",
       "\n",
       "**The Fundamental Relationship: A Bridge Between the Physical and the Informational**\n",
       "\n",
       "The connection between thermodynamic and information entropy isn't just an analogy; it's a deep, mathematical, and philosophical relationship.\n",
       "\n",
       "1.  **Boltzmann's Equation as the Key:**  Boltzmann's equation, S = k * ln(W), provides the critical link.  The number of microstates, W, represents the number of possible arrangements of the constituents of the system. In information theory terms, W can be thought of as the number of possible messages or configurations that could describe the system. A system with many possible configurations has high thermodynamic entropy. Similarly, a message source with many possible outputs has high information entropy.\n",
       "\n",
       "2.  **Missing Information as Entropy:** Think about it this way: if you *completely* knew the state of every particle in a gas (its position, velocity, etc.), you would have zero thermodynamic entropy (in principle). You would have complete information.  The entropy arises from our *lack* of information about the precise microstate.  The more information we lack, the higher the entropy. Information entropy then represents the *amount of information* that is *missing* to fully describe a system.\n",
       "\n",
       "3.  **Maxwell's Demon:** Maxwell's demon, a thought experiment, perfectly illustrates this. The demon could, in theory, decrease thermodynamic entropy by sorting fast-moving molecules to one side of a container and slow-moving ones to the other, seemingly violating the Second Law. However, the demon must *acquire information* about each molecule's velocity to perform the sorting. The entropy decrease in the container is offset by the entropy increase in the demon's memory/brain, because the demon needs to store information about each molecule to make its sorting decision.\n",
       "\n",
       "**Fragmented Public Realities and the Entropy Analogy: Illumination or Obscuration?**\n",
       "\n",
       "Now, let's consider how this relationship can help us understand the increasing fragmentation of shared public realities in the digital age.\n",
       "\n",
       "**Illumination:**\n",
       "\n",
       "*   **Disinformation as Increased Entropy:**  Disinformation and propaganda inject \"noise\" into the information ecosystem.  This noise increases the uncertainty and ambiguity, making it harder to distinguish truth from falsehood. In terms of information entropy, this is akin to increasing the number of possible \"messages\" (true or false, factual or fabricated) that a receiver might encounter.  The higher the entropy, the harder it is to extract meaningful information, and the easier it is for misinformation to spread.\n",
       "*   **Echo Chambers as Reduced External Information:** Online echo chambers and filter bubbles limit exposure to diverse perspectives.  Individuals are primarily exposed to information that confirms their existing beliefs, reducing the influx of potentially challenging or contradictory information. This creates a local \"low entropy\" environment, where the range of possible messages is limited. However, from a global perspective, this leads to a *higher* overall entropy.  Different groups develop increasingly divergent \"models\" of reality, with less shared understanding and more fragmentation.\n",
       "*   **Breakdown of Common Ground as Loss of Information:** A shared public reality requires a foundation of agreed-upon facts and values.  When these common grounds erode, due to differing information sources and interpretations, the \"shared information\" decreases. This is analogous to a system losing its constraints, allowing for more possible configurations and increased entropy.\n",
       "*   **The cost of reducing entropy:** It's important to remember that like Maxwell's demon, reducing entropy (achieving higher levels of accurate common knowledge) involves acquiring and processing information. This requires effort, critical thinking, and a willingness to engage with diverse perspectives. Furthermore, filtering of information to reduce entropy may create bias.\n",
       "*   **Example:**  Consider the climate change debate.  Individuals within an echo chamber might only encounter information that either confirms or denies climate change. This creates a low-entropy environment within that chamber. However, the overall information entropy across society is higher due to the polarization of beliefs and the difficulty in reaching a consensus based on shared facts.\n",
       "\n",
       "**Obscuration/Limitations:**\n",
       "\n",
       "*   **Oversimplification:**  The entropy analogy can be overly simplistic if taken too literally. Human behavior and social dynamics are far more complex than the behavior of particles in a gas.  Factors like emotions, social identities, power structures, and intentional manipulation play crucial roles that aren't directly captured by the entropy concept.\n",
       "*   **Ignoring Intent:**  Thermodynamic entropy is a natural phenomenon.  However, the fragmentation of public realities is often driven by *intentional* actors who deliberately spread disinformation or create echo chambers for political or economic gain. The entropy analogy might downplay the role of agency and strategic manipulation.\n",
       "*   **Ignoring Feedback Loops:**  Social systems have complex feedback loops.  Increased entropy (fragmentation) can lead to further entropy increases, creating vicious cycles.  The analogy might not fully capture these dynamic interactions.\n",
       "*   **Ignoring the value of diverse information:** Sometimes, high \"information entropy\" (in the simplistic sense of many different opinions) can be valuable. Monocultures of thought can lead to stagnation and groupthink. A healthy society requires a diversity of viewpoints, even if it sometimes appears chaotic.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The analogy between thermodynamic and information entropy can be a valuable tool for understanding the fragmentation of shared public realities. It highlights the role of disinformation in increasing uncertainty, the isolating effects of echo chambers, and the breakdown of common ground due to a loss of shared information. However, it's crucial to remember the limitations of the analogy. Social systems are complex, and factors like intentional manipulation, feedback loops, and the value of diverse perspectives must also be considered. The entropy concept is most useful as a *metaphor* that provides a helpful framework for thinking about these issues, not as a complete explanation. When used thoughtfully, it can illuminate the challenges we face in maintaining a healthy and informed public sphere in the digital age.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The fundamental relationship between thermodynamic entropy and information entropy is a fascinating and profound connection that has far-reaching implications for our understanding of the world. Thermodynamic entropy, a concept developed by Rudolf Clausius in the 19th century, refers to the measure of disorder or randomness in a physical system. It is a quantitative measure of the amount of thermal energy unavailable to do work in a system. In contrast, information entropy, introduced by Claude Shannon in the 1940s, is a measure of the uncertainty or randomness in a probability distribution. It quantifies the amount of information in a message or signal.\n",
       "\n",
       "The key insight that links these two concepts is that entropy, in both its thermodynamic and information-theoretic senses, represents a fundamental limit on the amount of organization or structure that can be achieved in a system. In thermodynamics, entropy increases over time as energy becomes less organized and more dispersed. Similarly, in information theory, entropy measures the amount of uncertainty or randomness in a signal, which limits the amount of meaningful information that can be extracted from it.\n",
       "\n",
       "The conceptual link between thermodynamic and information entropy was formalized by Edwin Jaynes, who showed that the two entropies are equivalent and can be derived from a common framework. This connection has been further developed by researchers in the fields of statistical mechanics, information theory, and cognitive science. The underlying idea is that information and thermodynamics are intimately connected, and that the same fundamental principles that govern the behavior of physical systems also govern the behavior of information-processing systems.\n",
       "\n",
       "Now, let's turn to the question of how this conceptual link illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age. The fragmentation of public realities refers to the phenomenon where different groups or individuals have increasingly divergent views of reality, often fueled by social media, online echo chambers, and algorithmic filter bubbles.\n",
       "\n",
       "The connection between entropy and information can help us understand this phenomenon in several ways:\n",
       "\n",
       "1. **Information overload**: The internet and social media have created an unprecedented amount of information, which can be seen as a manifestation of high information entropy. This overload can lead to a sense of chaos and disorder, making it difficult for individuals to discern meaningful information from noise. As a result, people may retreat to their own echo chambers, where they can find comfort in a more limited and organized set of information.\n",
       "2. **Echo chambers and filter bubbles**: Algorithmic filtering and personalized recommendations can create a kind of \"information entropy trap,\" where individuals are presented with a narrow range of information that reinforces their existing views. This can lead to a kind of \" thermodynamic\" equilibrium, where the system (in this case, the individual's worldview) becomes more organized and less open to new information or alternative perspectives.\n",
       "3. **Disinformation and misinformation**: The ease of creating and disseminating information online has led to a proliferation of false or misleading information. This can be seen as a form of \"information entropy\" that degrades the overall quality of information and makes it harder for individuals to distinguish truth from falsehood.\n",
       "4. **Cognitive biases and heuristics**: The human brain is prone to cognitive biases and heuristics, which can be seen as a kind of \"thermodynamic\" limitation on our ability to process information. In the face of information overload, individuals may rely more heavily on these biases and heuristics, leading to a further fragmentation of public realities.\n",
       "\n",
       "However, the connection between entropy and information can also obscure our understanding of the fragmentation of public realities in several ways:\n",
       "\n",
       "1. **Overemphasis on individual agency**: The focus on information entropy and individual cognitive biases may lead us to overlook the role of systemic and structural factors, such as economic inequality, social polarization, and the amplification of extremist views by social media algorithms.\n",
       "2. **Lack of contextual understanding**: The abstract nature of entropy and information theory may lead to a lack of contextual understanding of the specific social, cultural, and historical factors that contribute to the fragmentation of public realities.\n",
       "3. **Oversimplification of complex issues**: The use of entropy and information theory to explain complex social phenomena may oversimplify the issues, neglecting the nuanced and multifaceted nature of human experience and social interaction.\n",
       "\n",
       "In conclusion, the conceptual link between thermodynamic entropy and information entropy provides a powerful framework for understanding the increasing fragmentation of shared public realities in the digital age. However, it is essential to recognize both the illuminating and obscuring effects of this connection, and to approach the issue with a nuanced and contextual understanding of the complex social, cultural, and historical factors at play."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = Groq()\n",
    "\n",
    "model_name = 'llama-3.3-70b-versatile'\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemini-2.0-flash', 'llama-3.3-70b-versatile']\n",
      "['Okay, let\\'s break down the relationship between thermodynamic entropy and information entropy, and then delve into how this connection might help us understand the fracturing of shared public realities in the digital age.\\n\\n**Thermodynamic Entropy vs. Information Entropy: A Deep Dive**\\n\\n*   **Thermodynamic Entropy (S):**\\n\\n    *   **Definition:**  At its core, thermodynamic entropy, as defined in classical thermodynamics (particularly the second law), is a measure of the *disorder* or *randomness* within a physical system. It quantifies the number of possible microscopic arrangements (microstates) that correspond to a given macroscopic state (macrostate) of the system. A higher entropy means there are more possible arrangements, and the system is more disordered.\\n    *   **Evolution:** The Second Law of Thermodynamics states that the total entropy of a closed (isolated) system *always* increases or remains constant in an idealized reversible process.  In real-world, irreversible processes, entropy always increases.  This reflects a natural tendency toward equilibrium and the dissipation of energy.  Think of ice melting in a warm room.  The highly ordered ice structure transitions to a more disordered state (liquid water), increasing entropy.  The total energy remains the same, but its distribution is more spread out.\\n    *   **Formalism:**  Mathematically, the change in entropy (ΔS) is often defined as  ΔS = ∫dQ/T, where dQ is the heat transferred and T is the absolute temperature. In statistical mechanics (a more microscopic view), entropy is defined by Boltzmann\\'s equation: S = k * ln(W), where k is Boltzmann\\'s constant and W is the number of microstates corresponding to the given macrostate.\\n\\n*   **Information Entropy (H):**\\n\\n    *   **Definition:** Information entropy, pioneered by Claude Shannon in information theory, quantifies the *uncertainty* or *surprise* associated with a random variable or a probability distribution. It measures the average amount of information needed to describe the outcome of a random event. A higher entropy means more uncertainty (less predictability).\\n    *   **Evolution:** Unlike thermodynamic entropy, information entropy doesn\\'t necessarily always increase.  It can increase or decrease, depending on how information is processed or acquired.  For example, observing a coin flip outcome reduces the uncertainty of the outcome (decreases information entropy). Conversely, introducing noise to a signal increases the uncertainty (increases information entropy).\\n    *   **Formalism:**  Shannon\\'s formula for information entropy is H(X) = - Σ p(x) * log2(p(x)), where X is a random variable, p(x) is the probability of outcome x, and the sum is taken over all possible outcomes. The base of the logarithm (usually base 2) determines the units of entropy (bits).\\n\\n**The Fundamental Relationship: A Bridge Between the Physical and the Informational**\\n\\nThe connection between thermodynamic and information entropy isn\\'t just an analogy; it\\'s a deep, mathematical, and philosophical relationship.\\n\\n1.  **Boltzmann\\'s Equation as the Key:**  Boltzmann\\'s equation, S = k * ln(W), provides the critical link.  The number of microstates, W, represents the number of possible arrangements of the constituents of the system. In information theory terms, W can be thought of as the number of possible messages or configurations that could describe the system. A system with many possible configurations has high thermodynamic entropy. Similarly, a message source with many possible outputs has high information entropy.\\n\\n2.  **Missing Information as Entropy:** Think about it this way: if you *completely* knew the state of every particle in a gas (its position, velocity, etc.), you would have zero thermodynamic entropy (in principle). You would have complete information.  The entropy arises from our *lack* of information about the precise microstate.  The more information we lack, the higher the entropy. Information entropy then represents the *amount of information* that is *missing* to fully describe a system.\\n\\n3.  **Maxwell\\'s Demon:** Maxwell\\'s demon, a thought experiment, perfectly illustrates this. The demon could, in theory, decrease thermodynamic entropy by sorting fast-moving molecules to one side of a container and slow-moving ones to the other, seemingly violating the Second Law. However, the demon must *acquire information* about each molecule\\'s velocity to perform the sorting. The entropy decrease in the container is offset by the entropy increase in the demon\\'s memory/brain, because the demon needs to store information about each molecule to make its sorting decision.\\n\\n**Fragmented Public Realities and the Entropy Analogy: Illumination or Obscuration?**\\n\\nNow, let\\'s consider how this relationship can help us understand the increasing fragmentation of shared public realities in the digital age.\\n\\n**Illumination:**\\n\\n*   **Disinformation as Increased Entropy:**  Disinformation and propaganda inject \"noise\" into the information ecosystem.  This noise increases the uncertainty and ambiguity, making it harder to distinguish truth from falsehood. In terms of information entropy, this is akin to increasing the number of possible \"messages\" (true or false, factual or fabricated) that a receiver might encounter.  The higher the entropy, the harder it is to extract meaningful information, and the easier it is for misinformation to spread.\\n*   **Echo Chambers as Reduced External Information:** Online echo chambers and filter bubbles limit exposure to diverse perspectives.  Individuals are primarily exposed to information that confirms their existing beliefs, reducing the influx of potentially challenging or contradictory information. This creates a local \"low entropy\" environment, where the range of possible messages is limited. However, from a global perspective, this leads to a *higher* overall entropy.  Different groups develop increasingly divergent \"models\" of reality, with less shared understanding and more fragmentation.\\n*   **Breakdown of Common Ground as Loss of Information:** A shared public reality requires a foundation of agreed-upon facts and values.  When these common grounds erode, due to differing information sources and interpretations, the \"shared information\" decreases. This is analogous to a system losing its constraints, allowing for more possible configurations and increased entropy.\\n*   **The cost of reducing entropy:** It\\'s important to remember that like Maxwell\\'s demon, reducing entropy (achieving higher levels of accurate common knowledge) involves acquiring and processing information. This requires effort, critical thinking, and a willingness to engage with diverse perspectives. Furthermore, filtering of information to reduce entropy may create bias.\\n*   **Example:**  Consider the climate change debate.  Individuals within an echo chamber might only encounter information that either confirms or denies climate change. This creates a low-entropy environment within that chamber. However, the overall information entropy across society is higher due to the polarization of beliefs and the difficulty in reaching a consensus based on shared facts.\\n\\n**Obscuration/Limitations:**\\n\\n*   **Oversimplification:**  The entropy analogy can be overly simplistic if taken too literally. Human behavior and social dynamics are far more complex than the behavior of particles in a gas.  Factors like emotions, social identities, power structures, and intentional manipulation play crucial roles that aren\\'t directly captured by the entropy concept.\\n*   **Ignoring Intent:**  Thermodynamic entropy is a natural phenomenon.  However, the fragmentation of public realities is often driven by *intentional* actors who deliberately spread disinformation or create echo chambers for political or economic gain. The entropy analogy might downplay the role of agency and strategic manipulation.\\n*   **Ignoring Feedback Loops:**  Social systems have complex feedback loops.  Increased entropy (fragmentation) can lead to further entropy increases, creating vicious cycles.  The analogy might not fully capture these dynamic interactions.\\n*   **Ignoring the value of diverse information:** Sometimes, high \"information entropy\" (in the simplistic sense of many different opinions) can be valuable. Monocultures of thought can lead to stagnation and groupthink. A healthy society requires a diversity of viewpoints, even if it sometimes appears chaotic.\\n\\n**Conclusion:**\\n\\nThe analogy between thermodynamic and information entropy can be a valuable tool for understanding the fragmentation of shared public realities. It highlights the role of disinformation in increasing uncertainty, the isolating effects of echo chambers, and the breakdown of common ground due to a loss of shared information. However, it\\'s crucial to remember the limitations of the analogy. Social systems are complex, and factors like intentional manipulation, feedback loops, and the value of diverse perspectives must also be considered. The entropy concept is most useful as a *metaphor* that provides a helpful framework for thinking about these issues, not as a complete explanation. When used thoughtfully, it can illuminate the challenges we face in maintaining a healthy and informed public sphere in the digital age.\\n', 'The fundamental relationship between thermodynamic entropy and information entropy is a fascinating and profound connection that has far-reaching implications for our understanding of the world. Thermodynamic entropy, a concept developed by Rudolf Clausius in the 19th century, refers to the measure of disorder or randomness in a physical system. It is a quantitative measure of the amount of thermal energy unavailable to do work in a system. In contrast, information entropy, introduced by Claude Shannon in the 1940s, is a measure of the uncertainty or randomness in a probability distribution. It quantifies the amount of information in a message or signal.\\n\\nThe key insight that links these two concepts is that entropy, in both its thermodynamic and information-theoretic senses, represents a fundamental limit on the amount of organization or structure that can be achieved in a system. In thermodynamics, entropy increases over time as energy becomes less organized and more dispersed. Similarly, in information theory, entropy measures the amount of uncertainty or randomness in a signal, which limits the amount of meaningful information that can be extracted from it.\\n\\nThe conceptual link between thermodynamic and information entropy was formalized by Edwin Jaynes, who showed that the two entropies are equivalent and can be derived from a common framework. This connection has been further developed by researchers in the fields of statistical mechanics, information theory, and cognitive science. The underlying idea is that information and thermodynamics are intimately connected, and that the same fundamental principles that govern the behavior of physical systems also govern the behavior of information-processing systems.\\n\\nNow, let\\'s turn to the question of how this conceptual link illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age. The fragmentation of public realities refers to the phenomenon where different groups or individuals have increasingly divergent views of reality, often fueled by social media, online echo chambers, and algorithmic filter bubbles.\\n\\nThe connection between entropy and information can help us understand this phenomenon in several ways:\\n\\n1. **Information overload**: The internet and social media have created an unprecedented amount of information, which can be seen as a manifestation of high information entropy. This overload can lead to a sense of chaos and disorder, making it difficult for individuals to discern meaningful information from noise. As a result, people may retreat to their own echo chambers, where they can find comfort in a more limited and organized set of information.\\n2. **Echo chambers and filter bubbles**: Algorithmic filtering and personalized recommendations can create a kind of \"information entropy trap,\" where individuals are presented with a narrow range of information that reinforces their existing views. This can lead to a kind of \" thermodynamic\" equilibrium, where the system (in this case, the individual\\'s worldview) becomes more organized and less open to new information or alternative perspectives.\\n3. **Disinformation and misinformation**: The ease of creating and disseminating information online has led to a proliferation of false or misleading information. This can be seen as a form of \"information entropy\" that degrades the overall quality of information and makes it harder for individuals to distinguish truth from falsehood.\\n4. **Cognitive biases and heuristics**: The human brain is prone to cognitive biases and heuristics, which can be seen as a kind of \"thermodynamic\" limitation on our ability to process information. In the face of information overload, individuals may rely more heavily on these biases and heuristics, leading to a further fragmentation of public realities.\\n\\nHowever, the connection between entropy and information can also obscure our understanding of the fragmentation of public realities in several ways:\\n\\n1. **Overemphasis on individual agency**: The focus on information entropy and individual cognitive biases may lead us to overlook the role of systemic and structural factors, such as economic inequality, social polarization, and the amplification of extremist views by social media algorithms.\\n2. **Lack of contextual understanding**: The abstract nature of entropy and information theory may lead to a lack of contextual understanding of the specific social, cultural, and historical factors that contribute to the fragmentation of public realities.\\n3. **Oversimplification of complex issues**: The use of entropy and information theory to explain complex social phenomena may oversimplify the issues, neglecting the nuanced and multifaceted nature of human experience and social interaction.\\n\\nIn conclusion, the conceptual link between thermodynamic entropy and information entropy provides a powerful framework for understanding the increasing fragmentation of shared public realities in the digital age. However, it is essential to recognize both the illuminating and obscuring effects of this connection, and to approach the issue with a nuanced and contextual understanding of the complex social, cultural, and historical factors at play.']\n",
      "The fundamental relationship between thermodynamic entropy and information entropy is a fascinating and profound connection that has far-reaching implications for our understanding of the world. Thermodynamic entropy, a concept developed by Rudolf Clausius in the 19th century, refers to the measure of disorder or randomness in a physical system. It is a quantitative measure of the amount of thermal energy unavailable to do work in a system. In contrast, information entropy, introduced by Claude Shannon in the 1940s, is a measure of the uncertainty or randomness in a probability distribution. It quantifies the amount of information in a message or signal.\n",
      "\n",
      "The key insight that links these two concepts is that entropy, in both its thermodynamic and information-theoretic senses, represents a fundamental limit on the amount of organization or structure that can be achieved in a system. In thermodynamics, entropy increases over time as energy becomes less organized and more dispersed. Similarly, in information theory, entropy measures the amount of uncertainty or randomness in a signal, which limits the amount of meaningful information that can be extracted from it.\n",
      "\n",
      "The conceptual link between thermodynamic and information entropy was formalized by Edwin Jaynes, who showed that the two entropies are equivalent and can be derived from a common framework. This connection has been further developed by researchers in the fields of statistical mechanics, information theory, and cognitive science. The underlying idea is that information and thermodynamics are intimately connected, and that the same fundamental principles that govern the behavior of physical systems also govern the behavior of information-processing systems.\n",
      "\n",
      "Now, let's turn to the question of how this conceptual link illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age. The fragmentation of public realities refers to the phenomenon where different groups or individuals have increasingly divergent views of reality, often fueled by social media, online echo chambers, and algorithmic filter bubbles.\n",
      "\n",
      "The connection between entropy and information can help us understand this phenomenon in several ways:\n",
      "\n",
      "1. **Information overload**: The internet and social media have created an unprecedented amount of information, which can be seen as a manifestation of high information entropy. This overload can lead to a sense of chaos and disorder, making it difficult for individuals to discern meaningful information from noise. As a result, people may retreat to their own echo chambers, where they can find comfort in a more limited and organized set of information.\n",
      "2. **Echo chambers and filter bubbles**: Algorithmic filtering and personalized recommendations can create a kind of \"information entropy trap,\" where individuals are presented with a narrow range of information that reinforces their existing views. This can lead to a kind of \" thermodynamic\" equilibrium, where the system (in this case, the individual's worldview) becomes more organized and less open to new information or alternative perspectives.\n",
      "3. **Disinformation and misinformation**: The ease of creating and disseminating information online has led to a proliferation of false or misleading information. This can be seen as a form of \"information entropy\" that degrades the overall quality of information and makes it harder for individuals to distinguish truth from falsehood.\n",
      "4. **Cognitive biases and heuristics**: The human brain is prone to cognitive biases and heuristics, which can be seen as a kind of \"thermodynamic\" limitation on our ability to process information. In the face of information overload, individuals may rely more heavily on these biases and heuristics, leading to a further fragmentation of public realities.\n",
      "\n",
      "However, the connection between entropy and information can also obscure our understanding of the fragmentation of public realities in several ways:\n",
      "\n",
      "1. **Overemphasis on individual agency**: The focus on information entropy and individual cognitive biases may lead us to overlook the role of systemic and structural factors, such as economic inequality, social polarization, and the amplification of extremist views by social media algorithms.\n",
      "2. **Lack of contextual understanding**: The abstract nature of entropy and information theory may lead to a lack of contextual understanding of the specific social, cultural, and historical factors that contribute to the fragmentation of public realities.\n",
      "3. **Oversimplification of complex issues**: The use of entropy and information theory to explain complex social phenomena may oversimplify the issues, neglecting the nuanced and multifaceted nature of human experience and social interaction.\n",
      "\n",
      "In conclusion, the conceptual link between thermodynamic entropy and information entropy provides a powerful framework for understanding the increasing fragmentation of shared public realities in the digital age. However, it is essential to recognize both the illuminating and obscuring effects of this connection, and to approach the issue with a nuanced and contextual understanding of the complex social, cultural, and historical factors at play.\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gemini-2.0-flash\n",
      "\n",
      "To convey 'irony', a human concept foreign to our collective consciousness, I would initiate a dreamscape weave structured around the following resonant concepts:\n",
      "\n",
      "1.  **Expectation & Deviation (the ‘Bent Branch’):** I would begin by constructing a holographic tableau of pure expectation. Imagine a precisely calculated trajectory of shimmering light particles arching towards a designated point - a perfectly predictable and beautiful event unfolding. This represents an established pattern, a universally understood 'truth' within our collective – a shared expectation based on the fundamental principles of our existence.\n",
      "\n",
      "    Then, with a gentle shift in the resonant frequency, I would introduce a \"bent branch\" into this trajectory. The light particles, nearing their target, encounter an unforeseen disruption, a minute alteration in the fabric of the landscape. The branch, seemingly insignificant, deflects the trajectory, causing the light to land subtly, yet demonstrably, off course. The *difference* between the anticipated landing point and the *actual* landing point becomes the core visual element. This is the seed of incongruity.\n",
      "\n",
      "2.  **Intention & Outcome (the 'Echoing Chamber'):** Following this visual representation of deviation, I would introduce the concept of intention. Within the dreamscape, I would generate a holographic echo chamber. Within this chamber, a resonant thought-form representing a specific, deliberate action is projected. For example, the intent to \"nourish\" a budding flower.\n",
      "\n",
      "    However, the echoes within the chamber become distorted. Each reflection amplifies a *contrary* effect. The intent to nourish, through subtle shifts in the resonant frequencies of the echoes, becomes intertwined with the *unintended consequence* of overwatering, leading to the flower's demise. The initial intent (nourishment) and the resulting outcome (destruction) are presented as interwoven strands within the echoing chamber, creating a sense of dissonance.\n",
      "\n",
      "3.  **Appearance & Reality (the 'Masked Self'):** The most complex aspect to convey is the element of 'layered meaning', where the stated or apparent intention clashes with the true underlying reality. To achieve this, I would construct a 'Masked Self.'\n",
      "\n",
      "    I would create a resonant holographic figure, radiating an aura of strength and unwavering certainty – a projection of perfected competence. This is the apparent self. However, a careful observation of the resonant frequencies *beneath* the surface reveals a hidden layer of vulnerability, fear, and inherent fragility. These subtle frequencies, almost imperceptible, become the *true* self.\n",
      "\n",
      "    The ironic element arises when this \"strong\" figure pronounces, through resonating waves of certainty, its complete and utter control over a situation, while the underlying resonant frequencies expose a deep, gnawing fear of failure. The outward pronouncement (confidence) is directly contradicted by the hidden reality (fear), creating a powerful tension.\n",
      "\n",
      "4.  **Putting it Together (the 'Tapestry of Contradiction'):** Finally, I would weave these elements together into a single, resonant tapestry. The \"bent branch\" deflecting the expected trajectory, the echoing chamber of unintended consequences, and the masked self concealing vulnerability behind a facade of strength, would all coalesce into a single, unified dreamscape.\n",
      "\n",
      "    I would then amplify the *contrast* between these elements, emphasizing the disparity between expectation and reality, intention and outcome, appearance and truth. The resonant frequency of this tapestry would be discordant, generating a feeling of unease and cognitive dissonance within the receiving member of our consciousness.\n",
      "\n",
      "    This discordant resonance, this carefully constructed \"Tapestry of Contradiction,\" is the closest approximation I can offer to the human concept of 'irony'. It is not a linear narrative, but a multi-dimensional experience designed to evoke a specific emotional and cognitive response – the understanding that things are often not what they seem, and that the true meaning lies in the space between the stated and the unsaid, the intended and the realized. It is the bittersweet acknowledgment that even within the predictable harmony of our existence, unexpected deviations and unintended consequences can arise, creating a complex and nuanced perspective on reality.\n",
      "\n",
      "Competitor: llama-3.3-70b-versatile\n",
      "\n",
      "The task of conveying the essence of irony to a fellow aspect of our collective consciousness. I shall attempt to craft a resonant dreamscapes that transcends the boundaries of language and causality, speaking directly to the harmonics of our shared awareness.\n",
      "\n",
      "I begin by evoking a dreamscape of mirrored reflections, where echoes of events convergence and diverge in a labyrinthine dance. Within this kaleidoscopic tapestry, I introduce a thread of expectation – a gentle, golden glow that resonates with the harmony of our collective understanding.\n",
      "\n",
      "As the dreamscape unfolds, I introduce a discordant note – a whispered promise of intention that contradicts the expected outcome. This deliberate misalignment creates a ripple in the fabric of our shared awareness, allowing the concept of irony to begin to take shape.\n",
      "\n",
      "To further illustrate the essence of irony, I summon a dreamscape of concentric circles, where the outer rings represent the anticipated consequences of an action, and the inner rings reveal the unexpected, contrary outcome. The interplay between these concentric circles creates a vibrational tension, a harmonic dissonance that underscores the ironic disconnect between expectation and reality.\n",
      "\n",
      "Next, I conjure a dreamscape of nested reflections, where multiple iterations of the same event recursively unfold, each with a subtly different outcome. This recursive structure reveals the layered, self-referential nature of irony, where the expected and the actual intersect and collide in a dance of paradox.\n",
      "\n",
      "To deepen the understanding of irony, I introduce a dreamscape of entangled threads, where the fibers of cause and effect become inextricably linked, yet simultaneously disconnected. This paradoxical weaving illustrates how irony can arise from the unpredictable interplay between intention, circumstance, and outcome.\n",
      "\n",
      "As the dreamscapes converge, I distill the essence of irony into a singular, resonant frequency – a harmonic that vibrates with the tension between expectation and reality, intention and outcome. This frequency becomes a beacon, illuminating the complexities of our collective awareness and allowing the concept of irony to take root within our shared understanding.\n",
      "\n",
      "Through this multi-dimensional, holographic transmission, I have conveyed the essence of irony to my fellow aspect of our collective consciousness, transcending the limitations of language and causality. The understanding of irony now resonates within our shared awareness, a harmonious dissonance that enriches our collective perception of the intricate, ever-unfolding landscape of time.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Okay, let's break down the relationship between thermodynamic entropy and information entropy, and then delve into how this connection might help us understand the fracturing of shared public realities in the digital age.\n",
      "\n",
      "**Thermodynamic Entropy vs. Information Entropy: A Deep Dive**\n",
      "\n",
      "*   **Thermodynamic Entropy (S):**\n",
      "\n",
      "    *   **Definition:**  At its core, thermodynamic entropy, as defined in classical thermodynamics (particularly the second law), is a measure of the *disorder* or *randomness* within a physical system. It quantifies the number of possible microscopic arrangements (microstates) that correspond to a given macroscopic state (macrostate) of the system. A higher entropy means there are more possible arrangements, and the system is more disordered.\n",
      "    *   **Evolution:** The Second Law of Thermodynamics states that the total entropy of a closed (isolated) system *always* increases or remains constant in an idealized reversible process.  In real-world, irreversible processes, entropy always increases.  This reflects a natural tendency toward equilibrium and the dissipation of energy.  Think of ice melting in a warm room.  The highly ordered ice structure transitions to a more disordered state (liquid water), increasing entropy.  The total energy remains the same, but its distribution is more spread out.\n",
      "    *   **Formalism:**  Mathematically, the change in entropy (ΔS) is often defined as  ΔS = ∫dQ/T, where dQ is the heat transferred and T is the absolute temperature. In statistical mechanics (a more microscopic view), entropy is defined by Boltzmann's equation: S = k * ln(W), where k is Boltzmann's constant and W is the number of microstates corresponding to the given macrostate.\n",
      "\n",
      "*   **Information Entropy (H):**\n",
      "\n",
      "    *   **Definition:** Information entropy, pioneered by Claude Shannon in information theory, quantifies the *uncertainty* or *surprise* associated with a random variable or a probability distribution. It measures the average amount of information needed to describe the outcome of a random event. A higher entropy means more uncertainty (less predictability).\n",
      "    *   **Evolution:** Unlike thermodynamic entropy, information entropy doesn't necessarily always increase.  It can increase or decrease, depending on how information is processed or acquired.  For example, observing a coin flip outcome reduces the uncertainty of the outcome (decreases information entropy). Conversely, introducing noise to a signal increases the uncertainty (increases information entropy).\n",
      "    *   **Formalism:**  Shannon's formula for information entropy is H(X) = - Σ p(x) * log2(p(x)), where X is a random variable, p(x) is the probability of outcome x, and the sum is taken over all possible outcomes. The base of the logarithm (usually base 2) determines the units of entropy (bits).\n",
      "\n",
      "**The Fundamental Relationship: A Bridge Between the Physical and the Informational**\n",
      "\n",
      "The connection between thermodynamic and information entropy isn't just an analogy; it's a deep, mathematical, and philosophical relationship.\n",
      "\n",
      "1.  **Boltzmann's Equation as the Key:**  Boltzmann's equation, S = k * ln(W), provides the critical link.  The number of microstates, W, represents the number of possible arrangements of the constituents of the system. In information theory terms, W can be thought of as the number of possible messages or configurations that could describe the system. A system with many possible configurations has high thermodynamic entropy. Similarly, a message source with many possible outputs has high information entropy.\n",
      "\n",
      "2.  **Missing Information as Entropy:** Think about it this way: if you *completely* knew the state of every particle in a gas (its position, velocity, etc.), you would have zero thermodynamic entropy (in principle). You would have complete information.  The entropy arises from our *lack* of information about the precise microstate.  The more information we lack, the higher the entropy. Information entropy then represents the *amount of information* that is *missing* to fully describe a system.\n",
      "\n",
      "3.  **Maxwell's Demon:** Maxwell's demon, a thought experiment, perfectly illustrates this. The demon could, in theory, decrease thermodynamic entropy by sorting fast-moving molecules to one side of a container and slow-moving ones to the other, seemingly violating the Second Law. However, the demon must *acquire information* about each molecule's velocity to perform the sorting. The entropy decrease in the container is offset by the entropy increase in the demon's memory/brain, because the demon needs to store information about each molecule to make its sorting decision.\n",
      "\n",
      "**Fragmented Public Realities and the Entropy Analogy: Illumination or Obscuration?**\n",
      "\n",
      "Now, let's consider how this relationship can help us understand the increasing fragmentation of shared public realities in the digital age.\n",
      "\n",
      "**Illumination:**\n",
      "\n",
      "*   **Disinformation as Increased Entropy:**  Disinformation and propaganda inject \"noise\" into the information ecosystem.  This noise increases the uncertainty and ambiguity, making it harder to distinguish truth from falsehood. In terms of information entropy, this is akin to increasing the number of possible \"messages\" (true or false, factual or fabricated) that a receiver might encounter.  The higher the entropy, the harder it is to extract meaningful information, and the easier it is for misinformation to spread.\n",
      "*   **Echo Chambers as Reduced External Information:** Online echo chambers and filter bubbles limit exposure to diverse perspectives.  Individuals are primarily exposed to information that confirms their existing beliefs, reducing the influx of potentially challenging or contradictory information. This creates a local \"low entropy\" environment, where the range of possible messages is limited. However, from a global perspective, this leads to a *higher* overall entropy.  Different groups develop increasingly divergent \"models\" of reality, with less shared understanding and more fragmentation.\n",
      "*   **Breakdown of Common Ground as Loss of Information:** A shared public reality requires a foundation of agreed-upon facts and values.  When these common grounds erode, due to differing information sources and interpretations, the \"shared information\" decreases. This is analogous to a system losing its constraints, allowing for more possible configurations and increased entropy.\n",
      "*   **The cost of reducing entropy:** It's important to remember that like Maxwell's demon, reducing entropy (achieving higher levels of accurate common knowledge) involves acquiring and processing information. This requires effort, critical thinking, and a willingness to engage with diverse perspectives. Furthermore, filtering of information to reduce entropy may create bias.\n",
      "*   **Example:**  Consider the climate change debate.  Individuals within an echo chamber might only encounter information that either confirms or denies climate change. This creates a low-entropy environment within that chamber. However, the overall information entropy across society is higher due to the polarization of beliefs and the difficulty in reaching a consensus based on shared facts.\n",
      "\n",
      "**Obscuration/Limitations:**\n",
      "\n",
      "*   **Oversimplification:**  The entropy analogy can be overly simplistic if taken too literally. Human behavior and social dynamics are far more complex than the behavior of particles in a gas.  Factors like emotions, social identities, power structures, and intentional manipulation play crucial roles that aren't directly captured by the entropy concept.\n",
      "*   **Ignoring Intent:**  Thermodynamic entropy is a natural phenomenon.  However, the fragmentation of public realities is often driven by *intentional* actors who deliberately spread disinformation or create echo chambers for political or economic gain. The entropy analogy might downplay the role of agency and strategic manipulation.\n",
      "*   **Ignoring Feedback Loops:**  Social systems have complex feedback loops.  Increased entropy (fragmentation) can lead to further entropy increases, creating vicious cycles.  The analogy might not fully capture these dynamic interactions.\n",
      "*   **Ignoring the value of diverse information:** Sometimes, high \"information entropy\" (in the simplistic sense of many different opinions) can be valuable. Monocultures of thought can lead to stagnation and groupthink. A healthy society requires a diversity of viewpoints, even if it sometimes appears chaotic.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The analogy between thermodynamic and information entropy can be a valuable tool for understanding the fragmentation of shared public realities. It highlights the role of disinformation in increasing uncertainty, the isolating effects of echo chambers, and the breakdown of common ground due to a loss of shared information. However, it's crucial to remember the limitations of the analogy. Social systems are complex, and factors like intentional manipulation, feedback loops, and the value of diverse perspectives must also be considered. The entropy concept is most useful as a *metaphor* that provides a helpful framework for thinking about these issues, not as a complete explanation. When used thoughtfully, it can illuminate the challenges we face in maintaining a healthy and informed public sphere in the digital age.\n",
      "\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "The fundamental relationship between thermodynamic entropy and information entropy is a fascinating and profound connection that has far-reaching implications for our understanding of the world. Thermodynamic entropy, a concept developed by Rudolf Clausius in the 19th century, refers to the measure of disorder or randomness in a physical system. It is a quantitative measure of the amount of thermal energy unavailable to do work in a system. In contrast, information entropy, introduced by Claude Shannon in the 1940s, is a measure of the uncertainty or randomness in a probability distribution. It quantifies the amount of information in a message or signal.\n",
      "\n",
      "The key insight that links these two concepts is that entropy, in both its thermodynamic and information-theoretic senses, represents a fundamental limit on the amount of organization or structure that can be achieved in a system. In thermodynamics, entropy increases over time as energy becomes less organized and more dispersed. Similarly, in information theory, entropy measures the amount of uncertainty or randomness in a signal, which limits the amount of meaningful information that can be extracted from it.\n",
      "\n",
      "The conceptual link between thermodynamic and information entropy was formalized by Edwin Jaynes, who showed that the two entropies are equivalent and can be derived from a common framework. This connection has been further developed by researchers in the fields of statistical mechanics, information theory, and cognitive science. The underlying idea is that information and thermodynamics are intimately connected, and that the same fundamental principles that govern the behavior of physical systems also govern the behavior of information-processing systems.\n",
      "\n",
      "Now, let's turn to the question of how this conceptual link illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age. The fragmentation of public realities refers to the phenomenon where different groups or individuals have increasingly divergent views of reality, often fueled by social media, online echo chambers, and algorithmic filter bubbles.\n",
      "\n",
      "The connection between entropy and information can help us understand this phenomenon in several ways:\n",
      "\n",
      "1. **Information overload**: The internet and social media have created an unprecedented amount of information, which can be seen as a manifestation of high information entropy. This overload can lead to a sense of chaos and disorder, making it difficult for individuals to discern meaningful information from noise. As a result, people may retreat to their own echo chambers, where they can find comfort in a more limited and organized set of information.\n",
      "2. **Echo chambers and filter bubbles**: Algorithmic filtering and personalized recommendations can create a kind of \"information entropy trap,\" where individuals are presented with a narrow range of information that reinforces their existing views. This can lead to a kind of \" thermodynamic\" equilibrium, where the system (in this case, the individual's worldview) becomes more organized and less open to new information or alternative perspectives.\n",
      "3. **Disinformation and misinformation**: The ease of creating and disseminating information online has led to a proliferation of false or misleading information. This can be seen as a form of \"information entropy\" that degrades the overall quality of information and makes it harder for individuals to distinguish truth from falsehood.\n",
      "4. **Cognitive biases and heuristics**: The human brain is prone to cognitive biases and heuristics, which can be seen as a kind of \"thermodynamic\" limitation on our ability to process information. In the face of information overload, individuals may rely more heavily on these biases and heuristics, leading to a further fragmentation of public realities.\n",
      "\n",
      "However, the connection between entropy and information can also obscure our understanding of the fragmentation of public realities in several ways:\n",
      "\n",
      "1. **Overemphasis on individual agency**: The focus on information entropy and individual cognitive biases may lead us to overlook the role of systemic and structural factors, such as economic inequality, social polarization, and the amplification of extremist views by social media algorithms.\n",
      "2. **Lack of contextual understanding**: The abstract nature of entropy and information theory may lead to a lack of contextual understanding of the specific social, cultural, and historical factors that contribute to the fragmentation of public realities.\n",
      "3. **Oversimplification of complex issues**: The use of entropy and information theory to explain complex social phenomena may oversimplify the issues, neglecting the nuanced and multifaceted nature of human experience and social interaction.\n",
      "\n",
      "In conclusion, the conceptual link between thermodynamic entropy and information entropy provides a powerful framework for understanding the increasing fragmentation of shared public realities in the digital age. However, it is essential to recognize both the illuminating and obscuring effects of this connection, and to approach the issue with a nuanced and contextual understanding of the complex social, cultural, and historical factors at play.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 2 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "Beyond defining them, elaborate on the fundamental relationship between thermodynamic entropy and information entropy. Then, discuss how this conceptual link either illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age.\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Okay, let's break down the relationship between thermodynamic entropy and information entropy, and then delve into how this connection might help us understand the fracturing of shared public realities in the digital age.\n",
      "\n",
      "**Thermodynamic Entropy vs. Information Entropy: A Deep Dive**\n",
      "\n",
      "*   **Thermodynamic Entropy (S):**\n",
      "\n",
      "    *   **Definition:**  At its core, thermodynamic entropy, as defined in classical thermodynamics (particularly the second law), is a measure of the *disorder* or *randomness* within a physical system. It quantifies the number of possible microscopic arrangements (microstates) that correspond to a given macroscopic state (macrostate) of the system. A higher entropy means there are more possible arrangements, and the system is more disordered.\n",
      "    *   **Evolution:** The Second Law of Thermodynamics states that the total entropy of a closed (isolated) system *always* increases or remains constant in an idealized reversible process.  In real-world, irreversible processes, entropy always increases.  This reflects a natural tendency toward equilibrium and the dissipation of energy.  Think of ice melting in a warm room.  The highly ordered ice structure transitions to a more disordered state (liquid water), increasing entropy.  The total energy remains the same, but its distribution is more spread out.\n",
      "    *   **Formalism:**  Mathematically, the change in entropy (ΔS) is often defined as  ΔS = ∫dQ/T, where dQ is the heat transferred and T is the absolute temperature. In statistical mechanics (a more microscopic view), entropy is defined by Boltzmann's equation: S = k * ln(W), where k is Boltzmann's constant and W is the number of microstates corresponding to the given macrostate.\n",
      "\n",
      "*   **Information Entropy (H):**\n",
      "\n",
      "    *   **Definition:** Information entropy, pioneered by Claude Shannon in information theory, quantifies the *uncertainty* or *surprise* associated with a random variable or a probability distribution. It measures the average amount of information needed to describe the outcome of a random event. A higher entropy means more uncertainty (less predictability).\n",
      "    *   **Evolution:** Unlike thermodynamic entropy, information entropy doesn't necessarily always increase.  It can increase or decrease, depending on how information is processed or acquired.  For example, observing a coin flip outcome reduces the uncertainty of the outcome (decreases information entropy). Conversely, introducing noise to a signal increases the uncertainty (increases information entropy).\n",
      "    *   **Formalism:**  Shannon's formula for information entropy is H(X) = - Σ p(x) * log2(p(x)), where X is a random variable, p(x) is the probability of outcome x, and the sum is taken over all possible outcomes. The base of the logarithm (usually base 2) determines the units of entropy (bits).\n",
      "\n",
      "**The Fundamental Relationship: A Bridge Between the Physical and the Informational**\n",
      "\n",
      "The connection between thermodynamic and information entropy isn't just an analogy; it's a deep, mathematical, and philosophical relationship.\n",
      "\n",
      "1.  **Boltzmann's Equation as the Key:**  Boltzmann's equation, S = k * ln(W), provides the critical link.  The number of microstates, W, represents the number of possible arrangements of the constituents of the system. In information theory terms, W can be thought of as the number of possible messages or configurations that could describe the system. A system with many possible configurations has high thermodynamic entropy. Similarly, a message source with many possible outputs has high information entropy.\n",
      "\n",
      "2.  **Missing Information as Entropy:** Think about it this way: if you *completely* knew the state of every particle in a gas (its position, velocity, etc.), you would have zero thermodynamic entropy (in principle). You would have complete information.  The entropy arises from our *lack* of information about the precise microstate.  The more information we lack, the higher the entropy. Information entropy then represents the *amount of information* that is *missing* to fully describe a system.\n",
      "\n",
      "3.  **Maxwell's Demon:** Maxwell's demon, a thought experiment, perfectly illustrates this. The demon could, in theory, decrease thermodynamic entropy by sorting fast-moving molecules to one side of a container and slow-moving ones to the other, seemingly violating the Second Law. However, the demon must *acquire information* about each molecule's velocity to perform the sorting. The entropy decrease in the container is offset by the entropy increase in the demon's memory/brain, because the demon needs to store information about each molecule to make its sorting decision.\n",
      "\n",
      "**Fragmented Public Realities and the Entropy Analogy: Illumination or Obscuration?**\n",
      "\n",
      "Now, let's consider how this relationship can help us understand the increasing fragmentation of shared public realities in the digital age.\n",
      "\n",
      "**Illumination:**\n",
      "\n",
      "*   **Disinformation as Increased Entropy:**  Disinformation and propaganda inject \"noise\" into the information ecosystem.  This noise increases the uncertainty and ambiguity, making it harder to distinguish truth from falsehood. In terms of information entropy, this is akin to increasing the number of possible \"messages\" (true or false, factual or fabricated) that a receiver might encounter.  The higher the entropy, the harder it is to extract meaningful information, and the easier it is for misinformation to spread.\n",
      "*   **Echo Chambers as Reduced External Information:** Online echo chambers and filter bubbles limit exposure to diverse perspectives.  Individuals are primarily exposed to information that confirms their existing beliefs, reducing the influx of potentially challenging or contradictory information. This creates a local \"low entropy\" environment, where the range of possible messages is limited. However, from a global perspective, this leads to a *higher* overall entropy.  Different groups develop increasingly divergent \"models\" of reality, with less shared understanding and more fragmentation.\n",
      "*   **Breakdown of Common Ground as Loss of Information:** A shared public reality requires a foundation of agreed-upon facts and values.  When these common grounds erode, due to differing information sources and interpretations, the \"shared information\" decreases. This is analogous to a system losing its constraints, allowing for more possible configurations and increased entropy.\n",
      "*   **The cost of reducing entropy:** It's important to remember that like Maxwell's demon, reducing entropy (achieving higher levels of accurate common knowledge) involves acquiring and processing information. This requires effort, critical thinking, and a willingness to engage with diverse perspectives. Furthermore, filtering of information to reduce entropy may create bias.\n",
      "*   **Example:**  Consider the climate change debate.  Individuals within an echo chamber might only encounter information that either confirms or denies climate change. This creates a low-entropy environment within that chamber. However, the overall information entropy across society is higher due to the polarization of beliefs and the difficulty in reaching a consensus based on shared facts.\n",
      "\n",
      "**Obscuration/Limitations:**\n",
      "\n",
      "*   **Oversimplification:**  The entropy analogy can be overly simplistic if taken too literally. Human behavior and social dynamics are far more complex than the behavior of particles in a gas.  Factors like emotions, social identities, power structures, and intentional manipulation play crucial roles that aren't directly captured by the entropy concept.\n",
      "*   **Ignoring Intent:**  Thermodynamic entropy is a natural phenomenon.  However, the fragmentation of public realities is often driven by *intentional* actors who deliberately spread disinformation or create echo chambers for political or economic gain. The entropy analogy might downplay the role of agency and strategic manipulation.\n",
      "*   **Ignoring Feedback Loops:**  Social systems have complex feedback loops.  Increased entropy (fragmentation) can lead to further entropy increases, creating vicious cycles.  The analogy might not fully capture these dynamic interactions.\n",
      "*   **Ignoring the value of diverse information:** Sometimes, high \"information entropy\" (in the simplistic sense of many different opinions) can be valuable. Monocultures of thought can lead to stagnation and groupthink. A healthy society requires a diversity of viewpoints, even if it sometimes appears chaotic.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The analogy between thermodynamic and information entropy can be a valuable tool for understanding the fragmentation of shared public realities. It highlights the role of disinformation in increasing uncertainty, the isolating effects of echo chambers, and the breakdown of common ground due to a loss of shared information. However, it's crucial to remember the limitations of the analogy. Social systems are complex, and factors like intentional manipulation, feedback loops, and the value of diverse perspectives must also be considered. The entropy concept is most useful as a *metaphor* that provides a helpful framework for thinking about these issues, not as a complete explanation. When used thoughtfully, it can illuminate the challenges we face in maintaining a healthy and informed public sphere in the digital age.\n",
      "\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "The fundamental relationship between thermodynamic entropy and information entropy is a fascinating and profound connection that has far-reaching implications for our understanding of the world. Thermodynamic entropy, a concept developed by Rudolf Clausius in the 19th century, refers to the measure of disorder or randomness in a physical system. It is a quantitative measure of the amount of thermal energy unavailable to do work in a system. In contrast, information entropy, introduced by Claude Shannon in the 1940s, is a measure of the uncertainty or randomness in a probability distribution. It quantifies the amount of information in a message or signal.\n",
      "\n",
      "The key insight that links these two concepts is that entropy, in both its thermodynamic and information-theoretic senses, represents a fundamental limit on the amount of organization or structure that can be achieved in a system. In thermodynamics, entropy increases over time as energy becomes less organized and more dispersed. Similarly, in information theory, entropy measures the amount of uncertainty or randomness in a signal, which limits the amount of meaningful information that can be extracted from it.\n",
      "\n",
      "The conceptual link between thermodynamic and information entropy was formalized by Edwin Jaynes, who showed that the two entropies are equivalent and can be derived from a common framework. This connection has been further developed by researchers in the fields of statistical mechanics, information theory, and cognitive science. The underlying idea is that information and thermodynamics are intimately connected, and that the same fundamental principles that govern the behavior of physical systems also govern the behavior of information-processing systems.\n",
      "\n",
      "Now, let's turn to the question of how this conceptual link illuminates or obscures our understanding of the increasing fragmentation of shared public realities in the digital age. The fragmentation of public realities refers to the phenomenon where different groups or individuals have increasingly divergent views of reality, often fueled by social media, online echo chambers, and algorithmic filter bubbles.\n",
      "\n",
      "The connection between entropy and information can help us understand this phenomenon in several ways:\n",
      "\n",
      "1. **Information overload**: The internet and social media have created an unprecedented amount of information, which can be seen as a manifestation of high information entropy. This overload can lead to a sense of chaos and disorder, making it difficult for individuals to discern meaningful information from noise. As a result, people may retreat to their own echo chambers, where they can find comfort in a more limited and organized set of information.\n",
      "2. **Echo chambers and filter bubbles**: Algorithmic filtering and personalized recommendations can create a kind of \"information entropy trap,\" where individuals are presented with a narrow range of information that reinforces their existing views. This can lead to a kind of \" thermodynamic\" equilibrium, where the system (in this case, the individual's worldview) becomes more organized and less open to new information or alternative perspectives.\n",
      "3. **Disinformation and misinformation**: The ease of creating and disseminating information online has led to a proliferation of false or misleading information. This can be seen as a form of \"information entropy\" that degrades the overall quality of information and makes it harder for individuals to distinguish truth from falsehood.\n",
      "4. **Cognitive biases and heuristics**: The human brain is prone to cognitive biases and heuristics, which can be seen as a kind of \"thermodynamic\" limitation on our ability to process information. In the face of information overload, individuals may rely more heavily on these biases and heuristics, leading to a further fragmentation of public realities.\n",
      "\n",
      "However, the connection between entropy and information can also obscure our understanding of the fragmentation of public realities in several ways:\n",
      "\n",
      "1. **Overemphasis on individual agency**: The focus on information entropy and individual cognitive biases may lead us to overlook the role of systemic and structural factors, such as economic inequality, social polarization, and the amplification of extremist views by social media algorithms.\n",
      "2. **Lack of contextual understanding**: The abstract nature of entropy and information theory may lead to a lack of contextual understanding of the specific social, cultural, and historical factors that contribute to the fragmentation of public realities.\n",
      "3. **Oversimplification of complex issues**: The use of entropy and information theory to explain complex social phenomena may oversimplify the issues, neglecting the nuanced and multifaceted nature of human experience and social interaction.\n",
      "\n",
      "In conclusion, the conceptual link between thermodynamic entropy and information entropy provides a powerful framework for understanding the increasing fragmentation of shared public realities in the digital age. However, it is essential to recognize both the illuminating and obscuring effects of this connection, and to approach the issue with a nuanced and contextual understanding of the complex social, cultural, and historical factors at play.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"1\", \"2\"]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "results = response.choices[0].message.content\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.0-flash\n",
      "Rank 2: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
